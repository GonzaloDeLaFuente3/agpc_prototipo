# agent/fragmentador.py
import uuid
import re
from datetime import datetime
from typing import List, Dict, Tuple
from agent.extractor import extraer_palabras_clave
from agent.temporal_parser import detectar_timestamps_fragmento
from agent.utils import normalizar_timestamp_para_guardar

def criterio_fragmentacion_semantica(texto: str, max_palabras: int = 300) -> List[str]:
    """
    Fragmenta una conversaci√≥n en bloques sem√°nticamente coherentes.
    Criterios de fragmentaci√≥n:
    1. Cambios de hablante (si se detectan patrones como "Juan:", "- Mar√≠a:")
    2. P√°rrafos largos (m√°s de max_palabras)
    3. Cambios de tema (detecci√≥n b√°sica por puntos/cambios abruptos)
    4. Separadores expl√≠citos (l√≠neas con "---", "***", etc.)
    """
    if not texto or not texto.strip():
        return []
    
    # Normalizar texto
    texto = texto.strip()
    
    # 1. Dividir por separadores expl√≠citos primero
    separadores_explicitos = re.split(r'\n[-*=]{3,}\n|\n\s*[-*=]{3,}\s*\n', texto)
    
    fragmentos_finales = []
    
    for bloque in separadores_explicitos:
        bloque = bloque.strip()
        if not bloque:
            continue
            
        # 2. Detectar cambios de hablante
        # Patrones: "Nombre:", "- Nombre:", "[Timestamp] Nombre:"
        patron_hablante = r'^(\s*-?\s*[A-Z√Å√â√ç√ì√ö][a-z√°√©√≠√≥√∫√±\s]+:|^\[\d+:\d+\]\s*[A-Z√Å√â√ç√ì√ö][a-z√°√©√≠√≥√∫√±\s]+:)'
        
        lineas = bloque.split('\n')
        fragmento_actual = []
        
        for linea in lineas:
            linea = linea.strip()
            if not linea:
                continue
                
            # Si detectamos cambio de hablante y ya tenemos contenido, crear fragmento
            if re.match(patron_hablante, linea) and fragmento_actual:
                texto_fragmento = '\n'.join(fragmento_actual).strip()
                if len(texto_fragmento.split()) > 10:  # M√≠nimo 10 palabras
                    fragmentos_finales.extend(_dividir_por_tama√±o(texto_fragmento, max_palabras))
                fragmento_actual = [linea]
            else:
                fragmento_actual.append(linea)
        
        # Agregar √∫ltimo fragmento del bloque
        if fragmento_actual:
            texto_fragmento = '\n'.join(fragmento_actual).strip()
            if len(texto_fragmento.split()) > 10:
                fragmentos_finales.extend(_dividir_por_tama√±o(texto_fragmento, max_palabras))
    
    # Si no se encontraron patrones especiales, fragmentar por p√°rrafos y tama√±o
    if not fragmentos_finales:
        fragmentos_finales = _dividir_por_parrafos_y_tama√±o(texto, max_palabras)
    
    # Limpieza final
    return [f.strip() for f in fragmentos_finales if f.strip() and len(f.split()) >= 5]

def _dividir_por_tama√±o(texto: str, max_palabras: int) -> List[str]:
    """Divide un texto por tama√±o m√°ximo de palabras, respetando frases completas."""
    palabras = texto.split()
    
    if len(palabras) <= max_palabras:
        return [texto]
    
    fragmentos = []
    inicio = 0
    
    while inicio < len(palabras):
        fin = min(inicio + max_palabras, len(palabras))
        
        # Buscar punto final cercano para corte natural
        if fin < len(palabras):
            for i in range(fin, max(inicio + int(max_palabras * 0.7), inicio + 10), -1):
                if i < len(palabras) and palabras[i-1].endswith('.'):
                    fin = i
                    break
        
        fragmento = ' '.join(palabras[inicio:fin])
        fragmentos.append(fragmento)
        inicio = fin
    
    return fragmentos

def _dividir_por_parrafos_y_tama√±o(texto: str, max_palabras: int) -> List[str]:
    """Divisi√≥n por p√°rrafos con control de tama√±o."""
    parrafos = [p.strip() for p in texto.split('\n\n') if p.strip()]
    
    if not parrafos:
        parrafos = [p.strip() for p in texto.split('\n') if p.strip()]
    
    fragmentos = []
    fragmento_actual = []
    palabras_actuales = 0
    
    for parrafo in parrafos:
        palabras_parrafo = len(parrafo.split())
        
        if palabras_actuales + palabras_parrafo <= max_palabras:
            fragmento_actual.append(parrafo)
            palabras_actuales += palabras_parrafo
        else:
            # Guardar fragmento actual
            if fragmento_actual:
                fragmentos.append('\n\n'.join(fragmento_actual))
            
            # Si el p√°rrafo es muy largo, dividirlo
            if palabras_parrafo > max_palabras:
                fragmentos.extend(_dividir_por_tama√±o(parrafo, max_palabras))
                fragmento_actual = []
                palabras_actuales = 0
            else:
                fragmento_actual = [parrafo]
                palabras_actuales = palabras_parrafo
    
    # Agregar √∫ltimo fragmento
    if fragmento_actual:
        fragmentos.append('\n\n'.join(fragmento_actual))
    
    return fragmentos

def fragmentar_conversacion(conversacion: Dict) -> List[Dict]:
    """
    Toma una conversaci√≥n completa y la fragmenta autom√°ticamente.
    """
    contenido = conversacion.get('contenido', '').strip()
    if not contenido:
        return []
    
    # Obtener fragmentos de texto
    textos_fragmentos = criterio_fragmentacion_semantica(contenido)
    
    fragmentos_con_metadata = []
    conversacion_id = str(uuid.uuid4())
    
    # TIMESTAMP BASE YA NORMALIZADO (viene de agregar_conversacion)
    timestamp_base_conversacion = conversacion.get('fecha')
    print(f"Timestamp base conversaci√≥n: {timestamp_base_conversacion}")
    
    for i, texto_fragmento in enumerate(textos_fragmentos):
        fragmento_id = str(uuid.uuid4())
        
        # Detectar timestamp espec√≠fico del fragmento
        timestamp_fragmento = detectar_timestamps_fragmento(
            texto_fragmento, 
            timestamp_base_conversacion
        )
        
        # NORMALIZAR TIMESTAMP DEL FRAGMENTO
        if timestamp_fragmento:
            timestamp_normalizado = normalizar_timestamp_para_guardar(timestamp_fragmento)
            timestamp_fragmento = timestamp_normalizado if timestamp_normalizado else timestamp_base_conversacion
            print(f"  ‚úÖ Fragmento {i+1} - timestamp espec√≠fico normalizado: {timestamp_fragmento}")
        elif timestamp_base_conversacion:
            # Heredar timestamp base (ya normalizado)
            timestamp_fragmento = timestamp_base_conversacion
            print(f"  üìã Fragmento {i+1} - hered√≥ timestamp base: {timestamp_fragmento}")
        
        # Determinar si es temporal basado en timestamp espec√≠fico
        es_temporal = bool(timestamp_fragmento)
        
        palabras_clave = extraer_palabras_clave(texto_fragmento)
        
        # Crear metadatos del fragmento MEJORADOS
        metadata_fragmento = {
            "fragmento_id": fragmento_id,
            "conversacion_id": conversacion_id,
            "conversacion_titulo": conversacion.get('titulo', 'Sin t√≠tulo'),
            "posicion_en_conversacion": i + 1,
            "total_fragmentos_conversacion": len(textos_fragmentos),
            "texto": texto_fragmento,
            "palabras_clave": palabras_clave,
            "timestamp": timestamp_fragmento,
            "timestamp_original_conversacion": timestamp_base_conversacion,  # NUEVO: preservar original
            "participantes": conversacion.get('participantes', []),
            "metadata_conversacion": conversacion.get('metadata', {}),
            "created_at": datetime.now().strftime('%Y-%m-%dT%H:%M:%S'),
            "es_temporal": es_temporal,
            "tipo_contexto": _detectar_tipo_fragmento(texto_fragmento, conversacion.get('metadata', {})),
            "tiene_timestamp_especifico": timestamp_fragmento != timestamp_base_conversacion  # NUEVO: flag
        }
        
        fragmentos_con_metadata.append({
            'id': fragmento_id,
            'metadata': metadata_fragmento
        })
    
    return fragmentos_con_metadata

def _detectar_tipo_fragmento(texto: str, metadata_conversacion: Dict) -> str:
    """Detecta el tipo de fragmento basado en contenido y contexto de conversaci√≥n."""
    texto_lower = texto.lower()
    
    # Si la conversaci√≥n ya tiene tipo, heredarlo como base
    tipo_base = metadata_conversacion.get('tipo', 'general')
    
    # PATRONES ESPEC√çFICOS AMPLIADOS
    patrones_especificos = {
        "decision": ["decidimos", "acordamos", "resolveremos", "la decisi√≥n", "se decidi√≥", 
                    "optamos", "elegimos", "determinamos"],
        "accion": ["hacer", "implementar", "ejecutar", "realizar", "completar", 
                "desarrollar", "crear", "construir", "establecer"],
        "pregunta": ["¬ø", "como", "c√≥mo", "qu√©", "cu√°ndo", "d√≥nde", "por qu√©", 
                    "cu√°l", "qui√©n", "cu√°nto"],
        "conclusion": ["en resumen", "para concluir", "finalmente", "en conclusi√≥n",
                    "resumiendo", "concluyendo"],
        "problema": ["problema", "issue", "bug", "error", "falla", "no funciona",
                    "dificultad", "obst√°culo", "inconveniente"],
        "tarea": ["tarea", "pendiente", "debe", "tengo que", "hay que",
                "asignar", "responsable", "deadline"],
        "evento": ["reuni√≥n", "meeting", "cita", "evento", "conferencia",
                "presentaci√≥n", "demo"],
        "temporalidad_fuerte": ["ma√±ana", "ayer", "hoy", "pr√≥ximo", "pasado",
                            "lunes", "martes", "mi√©rcoles", "jueves", "viernes"]
    }
    
    # Contar coincidencias por categor√≠a
    coincidencias = {}
    for tipo, palabras in patrones_especificos.items():
        count = sum(1 for palabra in palabras if palabra in texto_lower)
        if count > 0:
            coincidencias[tipo] = count
    
    # Si hay coincidencias, usar la m√°s fuerte
    if coincidencias:
        tipo_detectado = max(coincidencias.items(), key=lambda x: x[1])[0]
        
        # Casos especiales
        if tipo_detectado == "temporalidad_fuerte":
            return "temporal_especifico"
        else:
            return tipo_detectado
    
    return tipo_base